{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span detection using BERT\n",
    "It is kind of extractive summerization in Q/A systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bert for span detection task. Note that, we use special model of large BERT for QA\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer for question answering\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2040,  2626, 12390,  1998, 13707,  1029,   102, 12390,  1998,\n",
       "         13707,  2003,  1037, 10576,  2517,  2011,  2520,  8101,  2220,  1999,\n",
       "          2010,  2476,  2055,  2048,  2402,  2732,  1011,  4625, 10205,  1012,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the question and the context\n",
    "question = \"Who wrote Romeo and Juliet?\"\n",
    "context = \"Romeo and Juliet is a tragedy written by William Shakespeare early in his career about two young star-crossed lovers.\"\n",
    "\n",
    "# Tokenize the input, add special tokens ([CLS] and [SEP]), and convert to tensor\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities for start are: tensor([[-6.3058, -5.7577, -6.7796, -3.3953, -8.1165, -6.3404, -8.4627, -6.3057,\n",
      "          0.1943, -5.3829, -3.9351, -5.0365, -3.2273, -2.6362, -1.4974, -2.2578,\n",
      "          7.5604,  3.0763, -4.2326, -6.6545, -3.2653, -4.8068, -7.0875, -6.4545,\n",
      "         -6.8650, -6.7581, -7.6815, -7.6125, -5.6213, -6.3056, -6.3057]])\n",
      "The strat index is: tensor(16)\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted start and end positions\n",
    "with torch.no_grad():\n",
    "    res = model(input_ids)\n",
    "\n",
    "start_pos = torch.argmax(res.start_logits)\n",
    "end_pos = torch.argmax(res.end_logits)\n",
    "\n",
    "print(\"The probabilities for start are:\", res.start_logits)\n",
    "print(\"The strat index is:\", start_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "william shakespeare\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# Convert the token IDs back to tokens and join to get the answer\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start_pos:end_pos+1]))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumer services\n"
     ]
    }
   ],
   "source": [
    "# Another Example\n",
    "\n",
    "\n",
    "\n",
    "question = 'what services does Coles provied?'\n",
    "\n",
    "context = \"Coles Supermarkets Australia Pty Ltd, trading as Coles, is an Australian supermarket, retail and consumer services chain,\\\n",
    "             headquartered in Melbourne as part of the Coles Group. Founded in 1914 in Collingwood by George Coles, Coles operates 846[3] \\\n",
    "             supermarkets throughout Australia, including several now re-branded Bi-Lo Supermarkets. Coles has over 120,000 employees[3][4]\\\n",
    "              and accounts for around 27 per cent of the Australian market.[5] Coles' large head office site in Melbourne's inner south-east \\\n",
    "              has 4,000 employees of the workforce located inside.\\\n",
    "                Coles Online is the company's online shopping ('click & collect' and home delivery) service.\\\n",
    "              Between 1986 and 2006, Coles Supermarkets was a brand of Coles Myer, later Coles Group, prior to Wesfarmers purchasing Coles \\\n",
    "              Group in 2007. It became a subsidiary of Coles Group again after Wesfarmers spun-off the business in November 2018.[6]\\\n",
    "              In 2020, Coles changed its slogan to 'Value the Australian way'\"\n",
    "\n",
    "# Tokenize the input, add special tokens ([CLS] and [SEP]), and convert to tensor\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "\n",
    "# Get the predicted start and end positions\n",
    "with torch.no_grad():\n",
    "    res = model(input_ids)\n",
    "\n",
    "start_pos = torch.argmax(res.start_logits)\n",
    "end_pos = torch.argmax(res.end_logits)\n",
    "\n",
    "\n",
    "\n",
    "# Inference\n",
    "# Convert the token IDs back to tokens and join to get the answer\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start_pos:end_pos+1]))\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive Summerization for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is Coles?\n",
      "Answer: an Australian supermarket, retail and consumer services chain\n",
      "\n",
      "Question: What does it do?\n",
      "Answer: operates 846[3] supermarkets throughout Australia\n",
      "\n",
      "Question: When it is established?\n",
      "Answer: 1914\n",
      "\n",
      "Question: Is Mayer mentioned in the cotext (Yes/NO)? \n",
      "Answer: yes\n",
      "\n",
      "Question: Summerize the context\n",
      "Answer: Coles Supermarkets Australia Pty Ltd, trading as Coles, is an Australian supermarket, retail and consumer services chain, headquartered in Melbourne\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the context and the question\n",
    "questions = ['what is Coles?',\n",
    "            'What does it do?',\n",
    "            'When it is established?',\n",
    "            'Is Mayer mentioned in the cotext (Yes/NO)? ',\n",
    "            'Summerize the context'\n",
    "]\n",
    "\n",
    "context = \"Coles Supermarkets Australia Pty Ltd, trading as Coles, is an Australian supermarket, retail and consumer services chain,\\\n",
    "             headquartered in Melbourne as part of the Coles Group. Founded in 1914 in Collingwood by George Coles, Coles operates 846[3] \\\n",
    "             supermarkets throughout Australia, including several now re-branded Bi-Lo Supermarkets. Coles has over 120,000 employees[3][4]\\\n",
    "              and accounts for around 27 per cent of the Australian market.[5] Coles' large head office site in Melbourne's inner south-east \\\n",
    "              has 4,000 employees of the workforce located inside.\\\n",
    "                Coles Online is the company's online shopping ('click & collect' and home delivery) service.\\\n",
    "              Between 1986 and 2006, Coles Supermarkets was a brand of Coles Myer, later Coles Group, prior to Wesfarmers purchasing Coles \\\n",
    "              Group in 2007. It became a subsidiary of Coles Group again after Wesfarmers spun-off the business in November 2018.[6]\\\n",
    "              In 2020, Coles changed its slogan to 'Value the Australian way'\"\n",
    "\n",
    "\n",
    "# Format the input string: The convention for T5 is to prepend \"question: [your question] context: [your context]\" It is like prompting\n",
    "for question in questions:\n",
    "  input_str = f\"question: {question} context: {context}\"\n",
    "  inputs = tokenizer.encode(input_str, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "  answer_ids = model.generate(inputs, max_length=500, num_beams=4, early_stopping=True)\n",
    "  # print(answer_ids)\n",
    "\n",
    "  # Decode and print the answer\n",
    "  answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "  print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
